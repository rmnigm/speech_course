{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Automatic Speech Recognition I"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%capture pip_install_requirements_output\n",
        "%pip install --quiet --upgrade -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import urllib\n",
        "from collections import defaultdict\n",
        "from typing import List, Tuple, TypeVar, Optional, Iterable\n",
        "\n",
        "import arpa\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import seaborn as sns\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torchaudio\n",
        "from g2p_en import G2p\n",
        "\n",
        "data_directory = './week_04_data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n",
        "public_key = 'https://disk.yandex.ru/d/KqloT1zKr_2VaA'\n",
        "final_url = base_url + urllib.parse.urlencode(dict(public_key=public_key))\n",
        "response = requests.get(final_url)\n",
        "download_url = response.json()['href']\n",
        "!wget -O week_04_data.tar.gz \"{download_url}\"\n",
        "!mkdir -p week_04_data\n",
        "!tar -xf week_04_data.tar.gz -C week_04_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Acoustic features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_sample_rate = 16_000\n",
        "\n",
        "waveform, sample_rate = torchaudio.load(os.path.join(data_directory, 'babenko.wav'))\n",
        "\n",
        "resample_waveform = torchaudio.transforms.Resample(\n",
        "    sample_rate,\n",
        "    target_sample_rate,\n",
        ")\n",
        "\n",
        "wav_to_melspec = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=target_sample_rate,\n",
        "    n_mels=80,\n",
        ")\n",
        "\n",
        "melspec = wav_to_melspec(resample_waveform(waveform))[0]\n",
        "\n",
        "# plotting\n",
        "plt.figure(figsize=(10, 5))\n",
        "melspec_log = (torch.log(melspec + 1e-9)).numpy()\n",
        "ax = sns.heatmap(melspec_log, cmap='viridis', cbar_kws={'label': 'Power'})\n",
        "\n",
        "num_ticks = 20\n",
        "time_ticks = np.linspace(0, melspec_log.shape[1], num_ticks)\n",
        "time_labels = [f\"{t:.1f}\" for t in np.linspace(0, waveform.shape[1] / target_sample_rate, num_ticks)]\n",
        "ax.set_xticks(time_ticks)\n",
        "ax.set_xticklabels(time_labels)\n",
        "\n",
        "plt.xlabel('Time (seconds)')\n",
        "plt.ylabel('Mel Frequency Bands')\n",
        "plt.title('Mel Spectrogram')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Speech units"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = 'you will not be forced to learn machine learning'\n",
        "\n",
        "words = text.split(' ')\n",
        "\n",
        "graphemes = list(text)\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "grapheme_to_phoneme_model = G2p()\n",
        "phonemes = grapheme_to_phoneme_model(text)\n",
        "\n",
        "subword_model = spm.SentencePieceProcessor(model_file=os.path.join(data_directory, 'sentencepiece.bpe.model'))  # https://huggingface.co/facebook/s2t-small-librispeech-asr/blob/main/sentencepiece.bpe.model\n",
        "subwords = subword_model.EncodeAsPieces(text, enable_sampling=False)\n",
        "\n",
        "print(f'{text = }\\n{words = }\\n{graphemes = }\\n{phonemes = }\\n{subwords = }')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWFxDM0QRg5S"
      },
      "source": [
        "## Metrics\n",
        "\n",
        "Word (Character, Phoneme) Error Rate (WER/CER/PER) – are the most popular metrics, which try to approximate how we perceive errors in the speech we hear. We will:\n",
        "- learn to calculate each of these distances using the Levenstein distance\n",
        "- implement both a naive recursive version of the Levenstein algorithm as well as a more efficient dynamic-programming implementation\n",
        "- using implementation of Levenstein distance you wil compute ASR quality metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzbhK3XoRg5W"
      },
      "source": [
        "### Levenshtein distance\n",
        "\n",
        "Consider an iterable sequence of elements, such as word or characters. Assume that the following operations can be performed on the sequence:\n",
        "* **insertion**: cat → ca<font color='green'>s</font>t,\n",
        "* **deletion**: ca<font color='red'>s</font>t → cat,\n",
        "* **substitution**: c<font color='blue'>a</font>t → c<font color='blue'>u</font>t,\n",
        "\n",
        "and suppose they have equal **costs**. These operations are enough to translate an arbitrary sequence into a different arbitrary sequence. There are many ways in which we can transform (edit) the sequence into a difference sequence. Given two sequences A and B, our goal is to find the minimum number of edits which are needed to transform sequence A into sequence B. This is known as the **Levenstein Distance**.\n",
        "\n",
        "<!-- ### Algorithm definition -->\n",
        "\n",
        "**Levenshtein distance** – the minimum number insertions, deletions, and substitutions required to transform sequence A into sequence B.\n",
        "\n",
        "\n",
        "The Levenstein distance can be computed using the following recursive algorithm, known as the **Levenstein Algorithm**:\n",
        "\n",
        "$$\n",
        "\\mathrm{L}(a, b) =\n",
        "\\begin{cases}\n",
        "    |a|,& \\text{if } |b| = 0, ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ \\text{\\# second sequence is empty} \\\\\n",
        "    |b|,& \\text{if } |a| = 0, ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ \\text{\\# first sequence is empty} \\\\\n",
        "    \\mathrm{L}(\\mathrm{tail}(a), \\mathrm{tail}(b)),& \\text{if } \\mathrm{head}(a) = \\mathrm{head}(b), ~ ~ \\text{\\# first elements of two sequencies are equal} \\\\\n",
        "    1 + min \n",
        "    \\begin{cases} \n",
        "        \\mathrm{L}(\\mathrm{tail}(a), b), ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ \\text{\\# deletion from first sequence} \\\\ \n",
        "        \\mathrm{L}(a, \\mathrm{tail}(b)), ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ \\text{\\# insertion into first sequence} \\\\ \n",
        "        \\mathrm{L}(\\mathrm{tail}(a), \\mathrm{tail}(b)); ~ ~ ~ ~ \\text{\\# substitution}\n",
        "    \\end{cases} & \\text{, otherwise.}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "<!-- %%\n",
        "\\text{lev}(a,b) = \\left \\{\n",
        "  \\begin{aligned}\n",
        "    &\\text{len}(a), & \\text(del)& &&\\text{if}\\ \\text{len}(b)=0, \\\\\n",
        "    &\\text{len}(b), & \\text(ins)& &&\\text{if}\\ \\text{len}(a)=0, \\\\\n",
        "    &\\text{lev}(a[1:],b[1:]) & \\text(cor)& &&\\text{if}\\ a[0] = b[0], \\\\\n",
        "    &1+\\min\\left \\{\n",
        "  \\begin{aligned}\n",
        "    &\\text{lev}(a[1:],b) & \\text(del)\\\\\n",
        "    &\\text{lev}(a,b[1:]) & \\text(ins)\\\\\n",
        "    &\\text{lev}(a[1:],b[1:]) & \\text(sub/cor)\n",
        "  \\end{aligned} \\right. && &&\\text{otherwise}\n",
        "  \\end{aligned} \\right.\n",
        "%% -->\n",
        "\n",
        "As you can see, the Levenshtein distance is a metric in the mathematical sense (symmetry, positive certainty, triangle inequality).\n",
        "\n",
        "**Question: what is the complexity of this algorithm?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhx-VsvX7KNn"
      },
      "source": [
        "### Naive recursive implementation\n",
        "\n",
        "Let's try to implement the recursive algorithm described above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KS2v-84rRg5c"
      },
      "outputs": [],
      "source": [
        "def levenshtein_naive(a: Iterable, b: Iterable) -> int:\n",
        "    \"\"\"Recursive implementation of Levenshtein distance\n",
        "\n",
        "    :param a: Iterable\n",
        "    :param b: Iterable\n",
        "    :return distance: int\n",
        "    \"\"\"\n",
        "\n",
        "    #############################################\n",
        "    # <YOUR CODE>\n",
        "    #############################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egn0K00v7qFv"
      },
      "outputs": [],
      "source": [
        "# Assess algorithm correctness\n",
        "def run_tests(fn):\n",
        "    assert fn('kitten', 'sitten') == 1\n",
        "    assert fn('kitten', 'sit') == 4\n",
        "    assert fn('kitten', 'puppy') == 6\n",
        "    assert fn('bcabac', 'cabcab') == 3\n",
        "\n",
        "run_tests(levenshtein_naive)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6VXFGS4Rg5d"
      },
      "source": [
        "###  Wagner–Fischer algorithm\n",
        "\n",
        "The complexity of the naive recursive implementation of the Levenshtein distance algorithm is exponential. This is due to the fact that the distances for the same suffixes are recalculated more than once! This can be avoided if we cache the results of calculations in the form of a matrix of distances between suffixes (more conveniently, prefixes), and fill in this matrix iteratively. The resulting algorithm is named **Wagner–Fischer algorithm** and is an example of a __dynamic programming__ algorithm.\n",
        "\n",
        "The Wagner-Fisher algorithm is defined as follows:\n",
        "\n",
        "\n",
        "$$\n",
        "  \\mathrm{L}_{a,b}(i,j) = \\left \\{\n",
        "  \\begin{aligned}\n",
        "    &\\max(i,j), && &&\\text{if}\\ \\min(i,j)=0, \\\\\n",
        "    &\\min\\left \\{\n",
        "  \\begin{aligned}\n",
        "    &\\mathrm{L}_{a,b}(i-1,j)+1 & \\text(del)\\\\\n",
        "    &\\mathrm{L}_{a,b}(i,j-1)+1 & \\text(ins)\\\\\n",
        "    &\\mathrm{L}_{a,b}(i-1,j-1)+\\delta(a_i \\neq b_j) & \\text(sub)\n",
        "  \\end{aligned} \\right. && &&\\text{otherwise}\n",
        "  \\end{aligned} \\right.\n",
        "$$\n",
        "\n",
        "\n",
        "**Implement** the `levenshtein_distance_matrix` function, which returns **the distance matrix between the prefixes of the two sequences**. The lower-right element of this matrix is the distance between the prefixes that are equal to the original sequences.\n",
        "\n",
        "It is necessary to fill in this matrix line by line: for a new element of this matrix, it is enough to know only its neighbors to the left, top, and left-top.\n",
        "\n",
        "We will also prepend an element denoting an **empty prefix** to the sequences – this is done in order to initialize the initial boundary values (initialize the first row and the first column of the matrix with the index values).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1ro4K-cRg5g",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "def levenshtein_distance_matrix(a: Iterable, b: Iterable) -> np.ndarray:\n",
        "    \"\"\"Matrix implementation of Levenshtein distance\n",
        "\n",
        "    :param a: Iterable\n",
        "    :param b: Iterable\n",
        "    :return distance matrix: np.ndarray\n",
        "    \"\"\"\n",
        "    a = ['#'] + list(a)\n",
        "    b = ['#'] + list(b)\n",
        "    d = np.zeros((len(a), len(b)), dtype=int)\n",
        "\n",
        "    #############################################\n",
        "    # <YOUR CODE>\n",
        "    #############################################\n",
        "\n",
        "    return d\n",
        "\n",
        "def levenshtein_dp(a: Iterable, b: Iterable) -> int:\n",
        "    return levenshtein_distance_matrix(a, b)[-1, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "HrBsYeY8Rg5e",
        "outputId": "15f8149a-64fa-4a06-b53f-1fc82c8239ae"
      },
      "outputs": [],
      "source": [
        "# Auxiliary function for drawing this matrix:\n",
        "def plot_matrix(matrix, row_names, column_names, path=None, mods=None):\n",
        "    \"\"\"\n",
        "    :param matrix: np.array [n_rows, n_cols] levenstein distance matrix\n",
        "    :param row_names: Name of the row elements\n",
        "    :param column_names: Name of the column elements\n",
        "    :param path:\n",
        "    :param mods:\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    row_names = ['#'] + list(row_names)\n",
        "    column_names = ['#'] + list(column_names)\n",
        "    matrix = np.array(matrix)\n",
        "\n",
        "    plt.figure(figsize=(len(column_names) / 2, len(row_names) / 2))\n",
        "    plt.imshow(matrix, interpolation='nearest', cmap=plt.get_cmap('Blues'))\n",
        "    plt.title(\"Levenshtein prefix distances\")\n",
        "\n",
        "    r = 0 if max(map(len, row_names + column_names)) < 3 else 45\n",
        "    plt.gca().xaxis.tick_top()\n",
        "    plt.xticks(range(len(column_names)), column_names, fontsize=12, rotation=r)\n",
        "    plt.yticks(range(len(row_names)), row_names, fontsize=12, rotation=r)\n",
        "\n",
        "    for i in range(matrix.shape[0]):\n",
        "        for j in range(matrix.shape[1]):\n",
        "            kwargs = {\n",
        "                'color': \"white\" if matrix[i, j] > matrix.max() / 2 else \"black\",\n",
        "                'horizontalalignment': 'center'\n",
        "            }\n",
        "            plt.text(j, i, \"{:,}\".format(matrix[i, j]), **kwargs)\n",
        "\n",
        "    if path is not None:\n",
        "        for (i, j), mod in zip(path, mods):\n",
        "            colors = {\n",
        "                'same': '#888888',\n",
        "                'subst': '#0000ff',\n",
        "                'del': '#ff0000',\n",
        "                'insert': '#00ff00'\n",
        "            }\n",
        "\n",
        "            rect = patches.Rectangle(\n",
        "                (j - 0.45, i - 0.45), 0.9, 0.9,\n",
        "                edgecolor=colors[mod], facecolor='none', linewidth=2)\n",
        "            plt.gca().add_patch(rect)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_matrix([\n",
        "    [0, 1, 2, 3, 4],\n",
        "    [1, 0, 1, 2, 3],\n",
        "    [2, 1, 0, 1, 2],\n",
        "    [3, 2, 1, 1, 1]\n",
        "], 'cat', 'cast')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "GvDduKSONGPd",
        "outputId": "87ded22e-61c4-44bf-c8e8-d9fd7c679e89"
      },
      "outputs": [],
      "source": [
        "# first, second = 'sunday', 'saturday'\n",
        "first, second = 'elephant', 'relevant'\n",
        "plot_matrix(levenshtein_distance_matrix(first, second), first, second)\n",
        "\n",
        "def run_tests(fn):\n",
        "    assert fn('kitten', 'sitten') == 1\n",
        "    assert fn('kitten', 'sit') == 4\n",
        "    assert fn('kitten', 'puppy') == 6\n",
        "    assert fn('bcabac', 'cabcab') == 3\n",
        "\n",
        "    for _ in range(100):\n",
        "        first = \"\".join([random.choice('abc') for _ in range(random.choice(range(3, 10)))])\n",
        "        second = \"\".join([random.choice('abc') for _ in range(random.choice(range(3, 10)))])\n",
        "        assert fn(first, second) == levenshtein_naive(first, second)\n",
        "\n",
        "# lets check our implementation on random sequences\n",
        "run_tests(levenshtein_dp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8rhH2V8Rg5g"
      },
      "source": [
        "### Backtrace\n",
        "\n",
        "To understand what insertions, deletions and substitutions were made on the original sequence, you can do a backtrace on the resulting matrix. \n",
        "\n",
        "Let's consider the first sequence as the original one, abd we will call the deletions and inserts relative to it.\n",
        "\n",
        "**Implement** the `backtrace` function, based on the construction logic `levenshtein_distance_matrix`:\n",
        "* write the path to the variable `path` – the list of the coordinates of the matrix cells that lie on the optimal path through the matrix;\n",
        "* and in the `mods` variable, write down the modifications that we make on the original sequence:\n",
        "    * `same` - leaving the element unchanged\n",
        "    * `subst` - replacing the element\n",
        "    * `del` - deleting the element\n",
        "    * `insert` - inserting the element"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82jz2-58Rg5h",
        "outputId": "e1abaf17-5559-4c33-fd89-ce2d13c50fce"
      },
      "outputs": [],
      "source": [
        "plot_matrix([\n",
        "    [0, 1, 2, 3, 4],\n",
        "    [1, 0, 1, 2, 3],\n",
        "    [2, 1, 0, 1, 2],\n",
        "    [3, 2, 1, 1, 1]\n",
        "],\n",
        "    'cat', 'cast',\n",
        "    [(0, 0), (1, 1), (2, 2), (2, 3), (3, 4)],\n",
        "    ['same', 'same', 'same', 'insert', 'same'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyA2UYyVRg5i"
      },
      "outputs": [],
      "source": [
        "def backtrace(d : np.ndarray):\n",
        "    \"\"\"Backtrace for Levenstein Distance\n",
        "\n",
        "    :param d: Levenstein Distance matrix (np.ndarray)\n",
        "    :return path:\n",
        "    :return path:\n",
        "    \"\"\"\n",
        "    path = []\n",
        "    mods = []\n",
        "\n",
        "    ##########################################\n",
        "    # <YOUR CODE>\n",
        "    ##########################################\n",
        "\n",
        "    return path, mods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "ounybXvsRg5i",
        "outputId": "b1d777cc-c274-4624-b821-fc4fd0ac077a"
      },
      "outputs": [],
      "source": [
        "first, second = 'thursday tea', 'friday beer'\n",
        "\n",
        "path, mods = backtrace(levenshtein_distance_matrix(first, second))\n",
        "plot_matrix(levenshtein_distance_matrix(first, second), first, second, path, mods)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgOxEhsYRg5j"
      },
      "source": [
        "Let's try applying the Levenshtein distance to a sequence of words, not characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "first = \"вас не будут заставлять учить машинное обучение\".split()\n",
        "second = \"вас не будут force to учить machine learning\".split()\n",
        "path, mods = backtrace(levenshtein_distance_matrix(first, second))\n",
        "\n",
        "S = int((np.array(mods) == 'subst').sum())\n",
        "I = int((np.array(mods) == 'insert').sum())\n",
        "D = int((np.array(mods) == 'del').sum())\n",
        "print(f\"{S = }\\t{I = }\\t{D = }\")\n",
        "assert (S, I, D) == (3, 1, 0)\n",
        "\n",
        "plot_matrix(levenshtein_distance_matrix(first, second), first, second, path, mods)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDRTZquNRg5k"
      },
      "source": [
        "### Error Rate\n",
        "\n",
        "In this part you will use the Levenstein distance which you implemented in the first part to obtain an measure of __mistmatch__ or __error__ between a reference and an ASR hypothesis at the word, character and phone level.\n",
        "\n",
        "#### Theoretical Recap\n",
        "\n",
        "Suppose we have our reference sequence, relative to which we want to calculate the recognition error.\n",
        "\n",
        "Why do you think the Levenshtein distance is not suitable for measuring the quality of the ASR system because the number of tokens in sentence can be different. Therefore, we need to normalize the Levenshtein distance by the length of the reference. This years our minimum edit distance **rate**.\n",
        "\n",
        "Word (character, phoneme, morpheme, syllable) error rate can then be computed as:\n",
        "\n",
        "$$\n",
        "\\mathrm{WER} = \\frac{\\mathrm{S} + \\mathrm{I} + \\mathrm{D}}{\\mathrm{N}}, \\text{where:} \\\\\n",
        "\\text{S is the number of substitutions,} \\\\\n",
        "\\text{I is the number of insertions,} \\\\\n",
        "\\text{D is the number of deletions,} \\\\\n",
        "\\text{N is the length of reference.}\n",
        "$$\n",
        "\n",
        "We can assess the error rate between two sequences at multiple levels - the word level (WER), the character (letter) level (CER) and the phoneme level (PER). WER is the most strict, as even a partially correct word is considered incorrect. Phone Error rate is in some sense the most lenient, as is measures whether the reference and hypthesis \"sound\" the same. Note that error can be assessed at other levels, like morphemes, lexemes and syllables, for example. Which metric is appropriate depends on the choice of language and what is being measured.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90ZttrTNBn-h"
      },
      "source": [
        "#### WER vs CER vs PER\n",
        "\n",
        "Implement the `error_rate` function, which will calculate the prediction error for a given sequence of tokens (words, characters or phonemes) by formula above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwmhiTI6Rg5k"
      },
      "outputs": [],
      "source": [
        "def error_rate(reference: Iterable, predicted: Iterable) -> float:\n",
        "    assert len(reference) > 0\n",
        "\n",
        "    ######################################\n",
        "    # <YOUR CODE>\n",
        "    ######################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assess error rate function\n",
        "\n",
        "# Calculate WER and CER\n",
        "first = \"you will not be forced to learn machine learning\"\n",
        "second = \"you'll not be forced to learn my sheen learning\"\n",
        "\n",
        "wer = np.round(error_rate(first.split(), second.split()), 4)\n",
        "cer = np.round(error_rate(first, second), 4)\n",
        "\n",
        "# Calculate PER - we provide a phonetic transcription of the above sentences.\n",
        "first = grapheme_to_phoneme_model(first)\n",
        "second = grapheme_to_phoneme_model(second)\n",
        "print(first)\n",
        "print(second)\n",
        "per = np.round(error_rate(first, second), 4)\n",
        "\n",
        "print('Word Error Rate:', wer)\n",
        "print('Character Error Rate:', cer)\n",
        "print('Phone Error Rate:', per)\n",
        "\n",
        "assert np.allclose(wer, 0.4444, rtol=1e-5, atol=1e-5)\n",
        "assert np.allclose(cer, 0.1875, rtol=1e-5, atol=1e-5)\n",
        "assert np.allclose(per, 0.1316, rtol=1e-5, atol=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connectionist Temporal Classification (CTC)\n",
        "\n",
        "### Lecture recap\n",
        "\n",
        "#### Problem statement\n",
        "\n",
        "<!-- <p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1FFGXZsCgy-uQfCBp7F4w1gaJbIGv6CV2\" height=\"200px\" width=\"700px\">   -->\n",
        "<p style=\"text-align:center;\"><img src=\"https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_04_asr/images/spectrogram_to_text.png\" height=\"200px\" width=\"700px\">  \n",
        "\n",
        "Define a modified label sequence $\\omega'_{1:2L + 1}$:\n",
        "- add blanks to the beginning and the end of the original label sequence $\\omega_{1:L}$\n",
        "- insert blanks between every pair of labels\n",
        "\n",
        "<!-- <p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1CEhWtVYrSSkaRtEsJr5QwiH8lMaSQ_uN\" height=\"150px\" width=\"400px\"> -->\n",
        "<p style=\"text-align:center;\"><img src=\"https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_04_asr/images/grapheme_eps.png\" height=\"150px\" width=\"400px\">\n",
        "\n",
        "\n",
        "Define $\\alpha_t(s)$ as the probability of all paths of length $t$ which go through state $\\omega_s'$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Denote a sequence of **acoustic features** or **observations** as\n",
        "\n",
        "$$\n",
        "    \\mathbf{X}_{1:T} = \\{x_1, \\ldots, x_T\\}\n",
        "$$\n",
        "\n",
        "Define a mapping $\\mathcal{M}$ between words $\\mathbf{w}$ and speech units $\\omega_{1:L}$:\n",
        "\n",
        "$$\n",
        "    \\{\\omega^{(q)}_{1:L_q}\\}^Q_{q = 1} = \\mathcal{M}(\\mathbf{w})\n",
        "$$\n",
        "\n",
        "$$\n",
        "    \\{\\mathbf{w}^{(p)}\\}^P_{p = 1} = \\mathcal{M}^{-1}(\\omega_{1:L})\n",
        "$$\n",
        "\n",
        "For some choices of speech units this mapping is not 1-to-1 ($Q > 1$, $P > 1$). A possible pair of text (green) and speech units (yellow):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- <p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1HWD_SFZzids3Nz67BK_NQ5awkw6yUvLo\" height=\"200px\" width=\"600px\"> -->\n",
        "<p style=\"text-align:center;\"><img src=\"https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_04_asr/images/text_to_letters.png\" height=\"200px\" width=\"600px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Automated speech recognition (ASR) is a **discriminative** task $\\rightarrow$ \"Which sequence $\\mathbf{\\hat w}$ is likely given the audio?\":\n",
        "\n",
        "$$\n",
        "    \\mathbf{\\hat w} = \\mathcal{M}^{-1}(\\hat \\omega_{1:L}), \\quad \\hat \\omega_{1:L} = \\arg \\max_{\\hat \\omega_{1: L}} P(\\hat \\omega_{1:L} | \\mathbf{X}_{1: T}; \\theta),\n",
        "$$\n",
        "\n",
        "where $\\theta$ denotes the parameters of the model we are building to solve the problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Discriminative state-space models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How feature vectors $\\mathbf{X}_{1: T}$ and speech units $\\omega_{1:L}$ relate or **align** to each other? Two common approaches to constructing models which can align:\n",
        "- state-space models\n",
        "- neural attention mechanisms\n",
        "\n",
        "State-space models represent the space of various alignments in the form of a table (called **trellis**), the rows of which correspond to phonemes, and the columns are observed variables. One alignment is the path in this table from the upper left corner to the lower right.\n",
        "\n",
        "<!-- <p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1npycuLvYq_-3p_xd6bouR21tfVeOvMUd\" height=\"300px\" width=\"600px\"> -->\n",
        "<p style=\"text-align:center;\"><img src=\"https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_04_asr/images/ctc_trellis.png\" height=\"300px\" width=\"600px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Denote a set of all paths in trellis that map onto the phoneme sequence $\\omega_{1:L}$ as $\\mathcal{A}(\\omega_{1:L})$, and let $\\pi_{1:T} \\in \\mathcal{A}(\\omega_{1:L})$ be an element of this set. Then a discriminative state-space system models $P(\\omega_{1:L} | \\mathbf{X}_{1: T}; \\theta)$ as \n",
        "\n",
        "$$\n",
        "    P(\\omega_{1:L} | \\mathbf{X}_{1: T}; \\theta) = \\sum_{\\pi_{1:T} \\in \\mathcal{A}(\\omega_{1:L})} P(\\pi_{1:T} | \\mathbf{X}_{1:T}; \\theta)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imagine that we have a recurrent neural network parametrized with $\\theta$. The network outputs a distribution $P(z_t|x_t; \\theta)$ over possible speech units $\\omega$ for each frame $x_t$:\n",
        "\n",
        "<!-- <p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=153E-ailMiLPg3joPSx016lGv6S4vXVD2\" height=\"300px\" width=\"550px\"> -->\n",
        "<p style=\"text-align:center;\"><img src=\"https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_04_asr/images/ctc_rnn.png\" height=\"300px\" width=\"550px\">\n",
        "\n",
        "CTC is a discriminative state-space model defined as:\n",
        "    \n",
        "$$\n",
        "    P(\\omega_{1:L} | \\mathbf{X}_{1: T}; \\theta) = \\sum_{\\pi_{1:T} \\in \\mathcal{A}(\\omega_{1:L})} \\prod_{t = 1}^T P(z_t = \\pi_t| x_t; \\theta)\n",
        "$$\n",
        "    \n",
        "- CTC assumes all states conditionally independent\n",
        "- Alignment free -- does not need prior alignment for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CTC Forward-Backward Algorithm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Forward Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "    \\alpha_t(s) = P(\\omega_{1:s/2}, \\pi_t = \\omega_s' | \\mathbf{X}_{1:T}, \\theta) = \\sum_{\\pi_{1:t - 1} \\in \\mathcal{A}(\\omega_{1:s/2}), \\, \\pi_t = \\omega_s'}  P(\\pi_{1:t} | \\mathbf{X}_{1:T}, \\theta)\n",
        "$$\n",
        "\n",
        "Note that despite the fact that we have moved to the extended sequence $\\omega'$, we are still interested in maximizing the probability of alignments to the original sequence. And step $s$ in the new sequence corresponds to step $s/2$ in the old sequence (rounded to the bottom).\n",
        "\n",
        "The CTC forward algorithm recursively computes the forward variable $\\alpha_t(s)$.\n",
        "\n",
        "<!-- <p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1QaW0mJ9c3Z0KJVk3pUSyC_kS_pFC_QxS\" height=\"400px\" width=\"600px\">   -->\n",
        "<p style=\"text-align:center;\"><img src=\"https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_04_asr/images/ctc_trellis_alpha.png\" height=\"400px\" width=\"600px\">  \n",
        "\n",
        "**Initialization.** We allow all prefixes to start with either a blank ($\\epsilon$) or the first symbol in $\\omega_{1:L}$. Also note that $\\alpha_t(s) = 0,\\ \\forall s < (2L + 1) - 2(T - t) - 1$, because these variables correspond to states for which there are not enough time-steps left to complete the sequence.\n",
        "\n",
        "This gives us the following rules for initialization:\n",
        "\n",
        "$$\n",
        "  \\begin{aligned}\n",
        "    &\\alpha_t(0) = 0, \\forall t & \\\\\n",
        "    &\\alpha_1(1) = P(z_1 = \\epsilon | \\mathbf{X}_{1:T}), &\\\\\n",
        "    &\\alpha_1(2) = P(z_1 = \\omega^{'}_2 | \\mathbf{X}_{1:T}), &\\\\\n",
        "    &\\alpha_1(s) = 0,\\ \\forall s > 2 &\\\\\n",
        "    &\\alpha_t(s) = 0,\\ \\forall s < (2L + 1) - 2(T - t) - 1 &  \\text{top right zeros}\\\\\n",
        "  \\end{aligned}\n",
        "$$\n",
        "\n",
        "**Recursion.** \n",
        "\n",
        "$$\n",
        "  \\begin{aligned}\n",
        "    &\\alpha_t(s) = \\left \\{\n",
        "  \\begin{aligned}\n",
        "    &\\big(\\alpha_{t-1}(s) + \\alpha_{t-1}(s-1) \\big) P(z_t = \\omega^{'}_s | \\mathbf{X}_{1:T}) & \\text{if}\\ \\omega_s^{'} = \\epsilon\\ \\text{or}\\\n",
        "    \\omega_s^{'} = \\omega_{s-2}^{'} \\\\\n",
        "    &\\big(\\alpha_{t-1}(s) + \\alpha_{t-1}(s-1) + \\alpha_{t-1}(s-2)\\big) P(z_t = \\omega^{'}_s | \\mathbf{X}_{1:T}) & \\text{otherwise}\\\\\n",
        "  \\end{aligned} \\right. \n",
        "  \\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "<!-- <p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1Tre3oFHyjigpqG-GI1xVrOchZAMnRYBK\" height=\"250px\" width=\"650px\"> -->\n",
        "<p style=\"text-align:center;\"><img src=\"./images/ctc_alpha_step.png\" height=\"250px\" width=\"650px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Backward Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define $\\beta_t(s)$ as the probability of all valid alignments $\\omega'_{s:L}$ starting in state $\\omega_s'$:\n",
        "\n",
        "$$\n",
        "    \\beta_t(s) = P(\\omega_{s/2:L}, \\pi_t = \\omega'_s | \\mathbf{X}_{1:T}, \\theta) = \\sum_{\\pi_{t + 1:T} \\in \\mathcal{A}(\\omega_{s/2:L}), \\, \\pi_t = \\omega_s'} P(\\pi_{t + 1:T} | \\mathbf{X}_{1:T}, \\theta)\n",
        "$$\n",
        "\n",
        "The CTC backward algorithm recursively computes the backward variable $\\beta_t(s)$:\n",
        "\n",
        "<!-- <p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=11x3TGAzL2LWfO0ZKpPHegOvv8Iw6ZC0X\" height=\"400px\" width=\"600px\"> -->\n",
        "<p style=\"text-align:center;\"><img src=\"https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_04_asr/images/ctc_trellis_beta.png\" height=\"400px\" width=\"600px\">\n",
        "\n",
        "\n",
        "The formulas for backward algorithm are as follows:\n",
        "\n",
        "$$\n",
        "  \\begin{aligned}\n",
        "    &\\beta_T(2L+1) = 1 &\\\\\n",
        "    &\\beta_T(2L) = 1 & \\\\\n",
        "    &\\beta_T(s) = 0, \\forall s < 2L &\\\\\n",
        "    &\\beta_t(s) = 0,\\ \\forall s > 2t &\\\\\n",
        "    &\\beta_t(2L+2) = 0,\\ \\forall t  & \\text{bottom left zeros} \\\\\n",
        "    &\\beta_t(s) = \\left \\{\n",
        "  \\begin{aligned}\n",
        "    &\\big(\\beta_{t+1}(s) + \\beta_{t+1}(s+1) \\big) P(z_t = \\omega^{'}_s | \\mathbf{X}_{1:T}) & \\text{if}\\ \\omega_s^{'} = \\epsilon\\ \\text{or}\\\n",
        "    \\omega_s^{'} = \\omega_{s+2}^{'} \\\\\n",
        "    &\\big(\\beta_{t+1}(s) +\\beta_{t+1}(s+1) + \\beta_{t+1}(s+2)\\big) P(z_t = \\omega^{'}_s | \\mathbf{X}_{1:T}) & \\text{otherwise}\\\\\n",
        "  \\end{aligned} \\right. \n",
        "  \\end{aligned}\n",
        "$$\n",
        "\n",
        "<!-- <p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1h7OBZZ02dwZ1mDhRYh7yTy7-UW4NmbXm\" height=\"250px\" width=\"650px\">  -->\n",
        "<p style=\"text-align:center;\"><img src=\"https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_04_asr/images/ctc_beta_step.png\" height=\"350px\" width=\"600px\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Alignment and Loss Computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use your newfound knowledge of the CTC forward-backward algorithm to obtain a soft-alignment\n",
        "\n",
        "Remember, that the forward variable is computed as follows:\n",
        "\n",
        "The probability of all paths passing through a state $\\pi_t = \\omega_s'$ is the product of forward and backward variables:\n",
        "\n",
        "$$\n",
        "    \\alpha_t(s) \\beta_t(s) = \\sum_{\\pi_{1:T} \\in \\mathcal{A}(\\omega_{1:L}), \\,\\pi_t=\\omega_s'} P(\\pi_{1:T} | \\mathbf{X}_{1:T}, \\theta)\n",
        "$$\n",
        "\n",
        "Then, for any $t$, sum of all such products yields total probability:\n",
        "\n",
        "$$\n",
        "     \\sum_{s = 1}^{2 L + 1} \\alpha_t(s) \\beta_t(s) = P(\\omega_{1:L} | \\mathbf{X}_{1:T}, \\theta)\n",
        "$$\n",
        "\n",
        "We can also use normalized $\\alpha_t(s) \\beta_t(s)$ as a measure of **soft-alignment**:\n",
        "\n",
        "$$\n",
        "    \\text{align}_t(s) = \\frac{\\alpha_t(s) \\beta_t(s)}{\\sum_{s = 1}^{2 L + 1} \\alpha_t(s) \\beta_t(s)}\n",
        "$$\n",
        "\n",
        "You should get something like\n",
        "\n",
        "<!-- <p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1HAIl9UPReiFQ7dNOZFGfvUWDurFDBZYM\" height=\"300px\" width=\"800px\">  -->\n",
        "<p style=\"text-align:center;\"><img src=\"https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_04_asr/images/soft_align.png\" height=\"300px\" width=\"800px\"> \n",
        "\n",
        "$$\n",
        "  \\text{align}_t(s) = \\frac{\\alpha_t(s)\\beta_t(s)}{\\sum_{s}\\alpha_t(s)\\beta_t(s)}\n",
        "$$\n",
        "\n",
        "\n",
        "Doing the computation in probability space can be numerically unstable, so you should do it in Log-Space using the\n",
        "provided logsumexp operation. Remember to return to prob space at the end. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implement CTC Forward Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NEG_INF = utils.NEG_INF\n",
        "BLANK_SYMBOL = utils.BLANK_SYMBOL\n",
        "\n",
        "tokenizer = utils.CTCTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def forward_algorithm(sequence: List[int], matrix: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    :param sequence: a string converted to an index array by Tokenizer\n",
        "    :param matrix: A matrix of shape (K, T) with probability distributions over phonemes at each moment of time.\n",
        "    :return: the result of the forward pass of shape (2 * len(sequence) + 1, T)\n",
        "    \"\"\"\n",
        "    # Turn probs into log-probs\n",
        "    matrix = np.log(matrix)\n",
        "\n",
        "    blank = tokenizer.get_symbol_index(BLANK_SYMBOL)\n",
        "    mod_sequence = utils.modify_sequence(sequence, blank)\n",
        "\n",
        "    # Initialze\n",
        "    alphas = np.full([len(mod_sequence), matrix.shape[1]], NEG_INF)\n",
        "\n",
        "    for t in range(matrix.shape[1]):\n",
        "        for s in range(len(mod_sequence)):\n",
        "            # First Step\n",
        "            if t == 0:\n",
        "                ########################\n",
        "                # YOUR CODE HERE\n",
        "                ########################\n",
        "            # Upper diagonal zeros\n",
        "            elif ...: # CONDITION\n",
        "                ########################\n",
        "                # YOUR CODE HERE\n",
        "                ########################\n",
        "            else:\n",
        "                # Need to do this stabily\n",
        "                if s == 0:\n",
        "                    ########################\n",
        "                    # YOUR CODE HERE\n",
        "                    ########################\n",
        "                elif s == 1:\n",
        "                    ########################\n",
        "                    # YOUR CODE HERE\n",
        "                    ########################\n",
        "                else:\n",
        "                    ########################\n",
        "                    # YOUR CODE HERE HINT - THERE IS ANOTHER IFELSE\n",
        "                    ########################\n",
        "    return alphas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implement The CTC Backward Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def backward_algorithm(sequence: List[int], matrix: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    :param sequence: a string converted to an index array by Tokenizer\n",
        "    :param matrix: A matrix of shape (K, T) with probability distributions over phonemes at each moment of time.\n",
        "    :return: the result of the backward pass of shape (2 * len(sequence) + 1, T)\n",
        "    \"\"\"\n",
        "    matrix = np.log(matrix)\n",
        "    blank = tokenizer.get_symbol_index(BLANK_SYMBOL)\n",
        "    mod_sequence = utils.modify_sequence(sequence, blank)\n",
        "    betas = np.full([len(mod_sequence), matrix.shape[1]], NEG_INF)\n",
        "\n",
        "    for t in reversed(range(matrix.shape[1])):\n",
        "        for s in reversed(range(len(mod_sequence))):\n",
        "            # First Step\n",
        "            if t == matrix.shape[1] - 1:\n",
        "                ########################\n",
        "                # YOUR CODE HERE\n",
        "                ########################\n",
        "            # Lower Diagonal Zeros\n",
        "            elif :  # CONDITION\n",
        "                ########################\n",
        "                # YOUR CODE HERE\n",
        "                ########################\n",
        "            else:\n",
        "                if s == len(mod_sequence) - 1:\n",
        "                    ########################\n",
        "                    # YOUR CODE HERE\n",
        "                    ########################\n",
        "                elif s == len(mod_sequence) - 2:\n",
        "                    ########################\n",
        "                    # YOUR CODE HERE\n",
        "                    ########################\n",
        "                else:\n",
        "                    ########################\n",
        "                    # YOUR CODE HERE HINT - THERE IS ANOTHER IFELSE\n",
        "                    ########################\n",
        "    return betas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Obtain Soft-Alignment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def soft_alignment(labels_indices: List[int], matrix: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Returns the alignment coefficients for the input sequence\n",
        "    \"\"\"\n",
        "    alphas = forward_algorithm(labels_indices, matrix)\n",
        "    betas = backward_algorithm(labels_indices, matrix)\n",
        "\n",
        "    # Move from log space back to prob space\n",
        "    align = np.exp(alphas + betas)\n",
        "\n",
        "    # Normalize Alignment\n",
        "    align = align / np.sum(align, axis=0, keepdims=True)\n",
        "\n",
        "    return align"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!L\n",
        "# Test your implementation\n",
        "\n",
        "# Load numpy matrix, add axis [classes,time]\n",
        "matrix = np.loadtxt(os.path.join(data_directory, 'test_matrix.txt'))\n",
        "\n",
        "# Create label_sequence\n",
        "labels_indices = tokenizer.text_to_indices('there se ms no good reason for believing that twillc ange')\n",
        "\n",
        "align = soft_alignment(labels_indices, matrix)\n",
        "f, ax = plt.subplots(1, 2, dpi=75, figsize=(15, 5))\n",
        "\n",
        "im = ax[0].imshow(align, aspect='auto', cmap='viridis', interpolation='nearest')\n",
        "ax[0].set_title(\"Alignment\")\n",
        "ax[0].set_ylabel(\"Phonemes\")\n",
        "ax[0].set_xlabel(\"Time\")\n",
        "f.colorbar(im, ax=ax[0])\n",
        "\n",
        "im = ax[1].imshow(np.log(align), aspect='auto', cmap='viridis', interpolation='nearest')\n",
        "ax[1].set_title(\"Alignment in log scale\")\n",
        "ax[1].set_ylabel(\"Phonemes\")\n",
        "ax[1].set_xlabel(\"Time\")\n",
        "f.colorbar(im, ax=ax[1])\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "ref_align = np.loadtxt(os.path.join(data_directory, 'soft_alignment.txt'))\n",
        "assert np.allclose(ref_align, align)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementing a Decoder for CTC model (5 points)\n",
        "\n",
        "Before you can start having fun with a CTC ASR model, you first need to make sure that you can correctly \"decode\" or generate text from a working model. This can be done in two ways - using a Greedy Decoder, which is simple and fast, or using a Prefix Beam Search decoder, which is slower, but takes advantages of the fact that multiple plath though a CTC trellis can map to the sample sentence. In the following exercise you will implement both decoders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Greedy Best-Path Decoder (1 point)\n",
        "\n",
        "After we’ve trained the model, we’d like to use it to find a likely output for a given input. Your goal is to implement a Greedy Best-Path decoder. Remember than in CTC the joint distribution over states factors out into a product of marginals:\n",
        "\n",
        "$${\\tt P}(\\mathbf{z}_{1:T}|\\mathbf{X}_{1:T},\\mathbf{\\theta}) = \\prod_{t = 1}^T{\\tt P}(z_t|\\mathbf{X}_{1:T},\\mathbf{\\theta})$$\n",
        "\n",
        "We can take the most likely output at each time-step, which gives us the alignment with the highest probability:\n",
        "\n",
        "$$\\mathbf{\\pi}^*_{1:T} = \\arg \\max_{\\mathbf{\\pi}_{1:T} } \\prod_{t=1}^T {\\tt P}(z_t = \\pi_t|\\mathbf{X}_{1:T})$$\n",
        "\n",
        "Then merge repeats and remove blanks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def greedy_decoder(output: torch.Tensor, labels: List[torch.Tensor],\n",
        "                   label_lengths: List[int], collapse_repeated: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    :param output: torch.Tensor of Probs or Log-Probs of shape [batch, time, classes]\n",
        "    :param labels: list of label indices converted to torch.Tensors\n",
        "    :param label_lengths: list of label lengths (without padding)\n",
        "    :param collapse_repeated: whether the repeated characters should be deduplicated\n",
        "    :return: the result of the decoding and the target sequence\n",
        "    \"\"\"\n",
        "    blank_label = tokenizer.get_symbol_index(BLANK_SYMBOL)\n",
        "\n",
        "    # Get max classes\n",
        "    ########################\n",
        "    # YOUR CODE HERE\n",
        "    arg_maxes = ...\n",
        "    ########################\n",
        "\n",
        "    decodes = []\n",
        "    targets = []\n",
        "\n",
        "    # For targets and decodes remove repeats and blanks\n",
        "    for i, args in enumerate(arg_maxes):\n",
        "        decode = []\n",
        "        true_labels = labels[i][:label_lengths[i]].tolist()\n",
        "        targets.append(tokenizer.indices_to_text(true_labels))\n",
        "\n",
        "        # Remove repeats, then remove blanks\n",
        "        ########################\n",
        "        # YOUR CODE HERE\n",
        "        ########################\n",
        "\n",
        "        decodes.append(tokenizer.indices_to_text(decode))\n",
        "    return decodes, targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing the greedy decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load numpy matrix, make its shape be in the form of [batch, classes, time]\n",
        "matrix = np.loadtxt(os.path.join(data_directory, 'test_matrix.txt'))[np.newaxis, :, :]\n",
        "\n",
        "# Turn into Torch Tensor of shape [batch, time, classes]\n",
        "matrix = torch.Tensor(matrix).transpose(1, 2)\n",
        "\n",
        "# Convert indices into torch.Tensor\n",
        "labels_indices = torch.Tensor(tokenizer.text_to_indices('there seems no good reason for believing that it will change'))\n",
        "\n",
        "# Run the Decoder\n",
        "decodes, targets = greedy_decoder(matrix, [labels_indices], [len(labels_indices)])\n",
        "\n",
        "assert decodes[0] == 'there se ms no good reason for believing that twillc ange'\n",
        "assert targets[0] == 'there seems no good reason for believing that it will change'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prefix (Beam Search) Decoding With LM (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The greedy decoder doesn't take into account the fact that a single output can have many alignments. For example, imagine that the true label for a phoneme sequence is $[a]$. Assume that alignments $[a, a, \\epsilon]$ and $[a, a, a]$ individually have lower probability than the probability $[b, b, b]$, but the sum of their probabilities is higher. In this case, the greedy decoder would choose the wrong alignment $[b, b, b]$ and propose a wrong hypothesis $[b]$ instead of $[a]$.\n",
        "\n",
        "Prefix decoding considers probabilities of multiple paths and merges them. It can also add external language model.\n",
        "\n",
        "<!-- <p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1_X9NfoSe8HLKfAErDtr0rBsIxoejA1kq\" height=\"500px\" width=\"900px\">  -->\n",
        "<p style=\"text-align:center;\"><img src=\"https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_04_asr/images/beam_search.png\" height=\"500px\" width=\"900px\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prefix decoding algorithm has 3 nested loops:\n",
        "- over time - we extend prefixes up to T times\n",
        "- over prefixes in the beam\n",
        "- over possible extensions of a prefix\n",
        "\n",
        "Each prefix can be extended in three possible ways:\n",
        "- with a blank\n",
        "- with a repeating character\n",
        "- with a non-repeating character\n",
        "\n",
        "We must keep track of two probabilities per prefix:\n",
        "- The probability of prefix ending with blank $P_b(t, s)$. \n",
        "- The probability of prefix not ending with blank $P_{nb}(t, s)$\n",
        "\n",
        "Here $t$ denotes time step and $s$ denotes a prefix we got after $t$ time steps.\n",
        "\n",
        "We start with an empty string prefix: \n",
        "\n",
        "$$\n",
        "    P_b(0, \\text{\"\"}) = 1\n",
        "$$\n",
        "$$\n",
        "    P_{nb}(0, \\text{\"\"}) = 0\n",
        "$$\n",
        "\n",
        "If we extend $s$ with a blank, update the probability of ending with a blank:\n",
        "\n",
        "$$\n",
        "    P_b(t, s) = P(\\epsilon | x_t) \\cdot (P_b(t - 1, s) + P_{nb}(t - 1, s))\n",
        "$$\n",
        "\n",
        "The prefix $s$ is not updated because blanks are eliminated in the end.\n",
        "\n",
        "If we extend with a repeat character $c$, there are two options:\n",
        "1. The previous symbol is a blank, and now we extend the prefix\n",
        "2. The previous symbol is not a blank, so we don't extend the prefix (repeats are merged)\n",
        "\n",
        "In this case, the probability $P_{nb}$ is updated as follows:\n",
        "\n",
        "$$\n",
        "    P_{nb}(t, s + c) = P(c | x_t) \\cdot P_b(t - 1, s)\n",
        "$$\n",
        "$$\n",
        "    P_{nb}(t, s) = P(c | x_t) \\cdot P_{nb}(t - 1, s)\n",
        "$$\n",
        "\n",
        "Finally, consider extending $s$ at time $t$ with a non-repeat character. It can follow both blank and non-blank characters, so the probability $P_{nb}$ is updated as follows:\n",
        "\n",
        "$$\n",
        "    P_{nb}(t, s + c) = P(c | x_t) \\cdot (P_b(t - 1, s) + P_{nb}(t - 1, s))\n",
        "$$\n",
        "\n",
        "We may also want to apply a language model during decoding, but only in the case we have a new complete word. This happens when the current symbol is a non-repeat space. As CTC is a discriminative model, LMs can only be integrated as a heuristic:\n",
        "\n",
        "$$\n",
        "    \\mathbf{w}^* = \\arg \\max_\\mathbf{w} \\underbrace{P(\\mathbf{w} | \\mathbf{X}_{1:T})}_{\\text{CTC prob}} \\cdot \\underbrace{P(\\mathbf{w})^{\\alpha}}_{\\text{LM prob}} \\cdot \\underbrace{|\\mathbf{w}|^\\beta}_{\\text{Length correction}}\n",
        "$$\n",
        "\n",
        "The formula for an update of $P_{nb}$ when LM is used and the current symbol is a non-repeat space:\n",
        "\n",
        "$$\n",
        "    P_{nb}(t, s + c) = P_{\\text{LM}}(s)^\\alpha \\cdot |s|^\\beta \\cdot P(c | x_t) \\cdot (P_b(t - 1, s) + P_{nb}(t - 1, s))\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LanguageModel = TypeVar(\"LanguageModel\")\n",
        "# Helper function\n",
        "\n",
        "class Beam:\n",
        "    def __init__(self, beam_size: int) -> None:\n",
        "        self.beam_size = beam_size\n",
        "\n",
        "        fn = lambda : (NEG_INF, NEG_INF)\n",
        "        self.candidates = defaultdict(fn)\n",
        "        self.top_candidates_list = [(tuple(), (0.0, NEG_INF))]\n",
        "\n",
        "    def get_probs_for_prefix(self, prefix: str) -> Tuple[float, float]:\n",
        "        p_blank, p_not_blank = self.candidates[prefix]\n",
        "        return p_blank, p_not_blank\n",
        "\n",
        "    def update_probs_for_prefix(self, prefix: str, next_p_blank: float, next_p_not_blank: float) -> None:\n",
        "        self.candidates[prefix] = (next_p_blank, next_p_not_blank)\n",
        "\n",
        "    def update_top_candidates_list(self) -> None:\n",
        "        top_candidates = sorted(\n",
        "            self.candidates.items(),\n",
        "            key=lambda x: utils.logsumexp(*x[1]),\n",
        "            reverse=True\n",
        "        )\n",
        "        self.top_candidates_list = top_candidates[:self.beam_size]\n",
        "\n",
        "\n",
        "def calculate_probability_score_with_lm(lm: LanguageModel, prefix: str) -> float:\n",
        "    text = tokenizer.indices_to_text(prefix).upper().strip()    # Use upper case for LM and remove the trailing space\n",
        "    lm_prob = lm.log_p(text)\n",
        "    score = lm_prob / np.log10(np.e)    # Convert to natural log, as ARPA LM uses log10\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode(probs: np.ndarray, beam_size: int = 5, lm: Optional[LanguageModel] = None,\n",
        "           prune: float = 1e-5, alpha: float = 0.1, beta: float = 2):\n",
        "    \"\"\"\n",
        "    :param probs: A matrix of shape (T, K) with probability distributions over phonemes at each moment of time.\n",
        "    :param beam_size: the size of beams\n",
        "    :lm: arpa language model\n",
        "    :prune: the minimal probability for a symbol at which it can be added to a prefix\n",
        "    :alpha: the parameter to de-weight the LM probability\n",
        "    :beta: the parameter to up-weight the length correction term\n",
        "    :return: the prefix with the highest sum of probabilites P_blank and P_not_blank\n",
        "    \"\"\"\n",
        "    T, S = probs.shape\n",
        "    probs = np.log(probs)\n",
        "    blank = tokenizer.get_symbol_index(BLANK_SYMBOL)\n",
        "    space = tokenizer.get_symbol_index(\" \")\n",
        "    prune = NEG_INF if prune == 0.0 else np.log(prune)\n",
        "\n",
        "    beam = Beam(beam_size)\n",
        "    for t in range(T):\n",
        "        next_beam = Beam(beam_size)\n",
        "\n",
        "        for s in range(S):\n",
        "            p = probs[t, s]\n",
        "            if p < prune:    # Prune the vocab\n",
        "                continue\n",
        "\n",
        "            for prefix, (p_blank, p_not_blank) in beam.top_candidates_list:\n",
        "                if s == blank:\n",
        "                    p_b, p_nb = next_beam.get_probs_for_prefix(prefix)\n",
        "                    next_beam.update_probs_for_prefix(\n",
        "                        prefix=,  # YOUR CODE\n",
        "                        next_p_blank=,  # YOUR CODE\n",
        "                        next_p_not_blank=,  # YOUR CODE\n",
        "                    )\n",
        "                    continue\n",
        "\n",
        "                end_t = prefix[-1] if prefix else None\n",
        "                n_prefix = prefix + (s,)\n",
        "\n",
        "                if s == end_t:\n",
        "                    p_b, p_nb = next_beam.get_probs_for_prefix(n_prefix)\n",
        "                    next_beam.update_probs_for_prefix(\n",
        "                        prefix=,  # YOUR CODE\n",
        "                        next_p_blank=,  # YOUR CODE\n",
        "                        next_p_not_blank=,  # YOUR CODE\n",
        "                    )\n",
        "\n",
        "                    p_b, p_nb = next_beam.get_probs_for_prefix(prefix)\n",
        "                    next_beam.update_probs_for_prefix(\n",
        "                        prefix=,  # YOUR CODE\n",
        "                        next_p_blank=,  # YOUR CODE\n",
        "                        next_p_not_blank=,  # YOUR CODE\n",
        "                    )\n",
        "                elif s == space and end_t is not None and lm is not None:\n",
        "                    p_b, p_nb = next_beam.get_probs_for_prefix(n_prefix)\n",
        "                    score = calculate_probability_score_with_lm(lm, n_prefix)\n",
        "                    length = len(tokenizer.indices_to_text(prefix))\n",
        "\n",
        "                    next_beam.update_probs_for_prefix(\n",
        "                        prefix=,  # YOUR CODE\n",
        "                        next_p_blank=,  # YOUR CODE\n",
        "                        next_p_not_blank=,  # YOUR CODE\n",
        "                    )\n",
        "                else:\n",
        "                    p_b, p_nb = next_beam.get_probs_for_prefix(n_prefix)\n",
        "                    next_beam.update_probs_for_prefix(\n",
        "                        prefix=,  # YOUR CODE\n",
        "                        next_p_blank=,  # YOUR CODE\n",
        "                        next_p_not_blank=,  # YOUR CODE\n",
        "                    )\n",
        "\n",
        "        next_beam.update_top_candidates_list()\n",
        "        beam = next_beam\n",
        "\n",
        "    best = beam.top_candidates_list[0]\n",
        "    return best[0], -utils.logsumexp(*best[1])\n",
        "\n",
        "\n",
        "def beam_search_decoder(probs: np.ndarray, labels: List[List[int]], label_lengths: List[int],\n",
        "                        input_lengths: List[int], lm: LanguageModel, beam_size: int = 5,\n",
        "                        prune: float = 1e-3, alpha: float = 0.1, beta: float = 0.1):\n",
        "    probs = probs.cpu().detach().numpy()\n",
        "    decodes, targets = [], []\n",
        "\n",
        "    for i, prob in enumerate(probs):\n",
        "        targets.append(tokenizer.indices_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "        int_seq, _ = decode(prob[:input_lengths[i]], lm=lm, beam_size=beam_size, prune=prune, alpha=alpha, beta=beta)\n",
        "        decodes.append(tokenizer.indices_to_text(int_seq))\n",
        "\n",
        "    return decodes, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create LM\n",
        "alm = arpa.loadf(os.path.join(data_directory, '3-gram.pruned.1e-7.arpa'))[0]\n",
        "alm._unk = '<UNK>'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing prefix (beam search) decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load numpy matrix, add axis [batch, classes, time]\n",
        "matrix = np.loadtxt(os.path.join(data_directory, 'test_matrix.txt'))[np.newaxis, :, :]\n",
        "\n",
        "# Turn into Torch Tensor of shape [batch, time, classes]\n",
        "matrix = torch.Tensor(matrix).transpose(1, 2)\n",
        "\n",
        "labels_indices = torch.Tensor(tokenizer.text_to_indices('there seems no good reason for believing that it will change'))\n",
        "\n",
        "# Run the Decoder\n",
        "decodes, targets = beam_search_decoder(\n",
        "    matrix, [labels_indices], [len(labels_indices)], [matrix.size()[1]],\n",
        "    lm=None, beam_size=5, prune=1e-3, alpha=0.1, beta=0.3\n",
        ")\n",
        "\n",
        "# assert decodes[0] == 'there se ms no good reason for believing that twillc ange'  # greedy\n",
        "assert decodes[0] == 'there se ms no good reason for believing that twil c ange'\n",
        "assert targets[0] == 'there seems no good reason for believing that it will change'\n",
        "\n",
        "decodes, targets = beam_search_decoder(\n",
        "    matrix, [labels_indices], [len(labels_indices)], [matrix.size()[1]],\n",
        "    lm=alm, beam_size=5, prune=1e-3, alpha=0.1, beta=0.3\n",
        ")\n",
        "\n",
        "assert decodes[0] == 'there seems no good reason for believing that twil c ange'\n",
        "assert targets[0] == 'there seems no good reason for believing that it will change'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Examples\n",
        "\n",
        "- Jasper https://nvidia.github.io/OpenSeq2Seq/html/speech-recognition/jasper.html\n",
        "- DeepSpeech2 https://nvidia.github.io/OpenSeq2Seq/html/speech-recognition/deepspeech2.html\n",
        "- VGGTransformer https://github.com/facebookresearch/fairseq/blob/main/examples/speech_recognition/models/vggtransformer.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RNN-Transducer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lecture recap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Alignment\n",
        "\n",
        "Let $\\mathbf{x} = (x_1, x_2, \\ldots, x_T)$ be a length $T$ input sequence of arbitrary length beloging to the set $X^*$ of all sequences over some input space $X$. Let $\\mathbf{y} = (y_1, \\ldots, y_U)$ be a length $U$ output sequence belonging to the set $Y^*$ of all sequences over some output space $Y$.\n",
        "\n",
        "Define the *extended output space* $\\overline Y$ as $Y \\cup \\emptyset$, where $\\emptyset$ denotes the null output. The intuitive meaning of $\\emptyset$ is 'output nothing'. The sequence $(y_1, \\emptyset, \\emptyset, y_2, \\emptyset, y_3) \\in \\overline Y^*$ is therefore equivalent to $(y_1, y_2, y_3) \\in Y^*$. We refer to the elements $\\mathbf{a} \\in \\overline Y^*$ as *alignments*, since the location of the null symbols determines an alignment between the input and output sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we saw in CTC, various alignments can be represented in the form of a table called trellis. An example of how an RNN-T trellis may look like:\n",
        "\n",
        "<!-- <p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1CfXfkePAESz2n20AABVUw9SaZ_xszxwf\"> -->\n",
        "<p style=\"text-align:center;\"><img src=\"https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_04_asr/images/rnnt_trellis_1.png\">\n",
        "    \n",
        "    \n",
        "Possible alignments in that trellis:\n",
        "    \n",
        "<!-- <p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1ipRlSrznwmoD5gCk7k6G06JeUtqPzDQq\"> -->\n",
        "<p style=\"text-align:center;\"><img src=\"https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_04_asr/images/rnnt_trellis_2.png\">\n",
        "    \n",
        "The final label can be determined by simply removing the blank characher:\n",
        "    \n",
        "$$\n",
        "    C \\emptyset \\emptyset A \\emptyset T \\emptyset \\to CAT\n",
        "$$\n",
        "$$\n",
        "    \\emptyset \\emptyset \\emptyset C A T \\emptyset \\to CAT\n",
        "$$\n",
        "    \n",
        "Given $\\mathbf{x}$, the RNN transducer defines a conditional distribution $P(\\mathbf{a} \\in \\overline Y^* | \\mathbf{x})$. This distribution is then collapsed onto the following distribution over $Y^*$:\n",
        "    \n",
        "$$\n",
        "    P(\\mathbf y \\in Y^* | \\mathbf x) = \\sum_{\\mathbf a \\in \\mathcal{B}^{-1}(\\mathbf y)} P(\\mathbf a | \\mathbf x),\n",
        "$$\n",
        "    \n",
        "where $\\mathcal B: \\overline Y^* \\mapsto Y^*$ is a function that removes the null symbols from the alignments in $Y^*$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Architecture\n",
        "\n",
        "<!-- <p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1P2aztCi9Z7ookMbHmWBcGtSmG_JHIiMj\"> -->\n",
        "<p style=\"text-align:center;\"><img src=\"https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_04_asr/images/rnnt_arch.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The RNN-T model consists of three neural networks: Encoder, Predictor and Joiner. The Encoder converts the acoustic feature $x_t$ into a high-level representation $f_t$, where $t$ is time index:\n",
        "\n",
        "$$\n",
        "    f_t = \\mathrm{Encoder}(x_t)\n",
        "$$\n",
        "\n",
        "The Predictor works like an RNN language model, which produces a high-level representation $g_u$ by conditioning on the previous non-blank target $y_{u - 1}$ predicted by the RNN-T model, where $u$ is output label index:\n",
        "\n",
        "$$\n",
        "    g_u = \\mathrm{Predictor}(y_{u - 1})\n",
        "$$\n",
        "\n",
        "Note that the input sequence for the predictor **is prepended with the special symbol** $\\langle s \\rangle$ that defines the start of a sentence.\n",
        "\n",
        "The Joiner is a feed forward network that combines the Encoder output $f_t$ and the Predictor output $g_u$ as\n",
        "\n",
        "$$\n",
        "    h_{t, u} = \\mathrm{Joiner}(f_t, g_u) = \\mathrm{FeedForward}(\\mathrm{ReLU}(f_t + g_u))\n",
        "$$\n",
        "\n",
        "The final posterior for each output token $y$ is obtained after applying the softmax operation:\n",
        "\n",
        "$$\n",
        "    P(y | t, u) = \\mathrm{softmax}(h_{t, u})\n",
        "$$\n",
        "    \n",
        "where $P(y | t, u)$ is a distribution of probabilities to emit $y \\in \\overline Y$ at time step $t$ after $u$ previously generated characters, $t \\in [1, T], u \\in [0, U]$.\n",
        "\n",
        "<!-- <p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1tn1wS3fCVFJGwrYumf5Im6gOFZsxRMV-\"> -->\n",
        "<p style=\"text-align:center;\"><img src=\"./images/rnnt_probs.png\">\n",
        "\n",
        "We will further need to work with probabilities of individual tokens $y$ for different $t$ and $u$. Instead of writing each time something like $P(y = C | t = 1, u = 0)$, we will, for the sake of simplicity, write it as $P(C | 1, 0)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training: forward-backward algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The loss function of RNN-T is the negative log posterior of output label sequence $\\mathbf y$ given acoustic feature $\\mathbf x$:\n",
        "\n",
        "$$\n",
        "    \\mathcal L = -\\ln P(\\mathbf y \\in Y^* | \\mathbf x) = -\\ln \\sum_{\\mathbf a \\in \\mathcal{B}^{-1}(\\mathbf y)} P(\\mathbf a | \\mathbf x)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To determine $P(\\mathbf a | \\mathbf x)$ for an arbitrary alignment $\\mathbf a$, we need to multiply the probabilities $P(y | t, u)$ of each symbol across the path:\n",
        "\n",
        "<!-- <p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1O-aykP5Wods7ZESCJDBsBw2MeBo5egW4\"> -->\n",
        "<p style=\"text-align:center;\"><img src=\"https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_04_asr/images/rnnt_trellis_probs.png\">\n",
        "\n",
        "$$\n",
        "    \\mathbf a = C \\emptyset \\emptyset A \\emptyset T \\emptyset\n",
        "$$\n",
        "    \n",
        "$$\n",
        "    P(\\mathbf a | \\mathbf x) = P(C | 1, 0) \\cdot P(\\emptyset | 1, 0) \\cdot P(\\emptyset | 2, 1) \\cdot P(A | 3, 1) \\cdot P(\\emptyset | 3, 2) \\cdot P(T | 3, 2) \\cdot P(\\emptyset | 4, 3)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are usually too many possible alignments to compute the loss function by just adding them all up directly. We will use dynamic programming to make this computation feasible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the *forward variable* $\\alpha(t, u)$ as the probability of outputting $\\mathbf y_{[1:u]}$ during $\\mathbf f_{[1:t]}$. The forward variables for all $1 \\le t \\le T$ and $0 \\le u \\le U$ can be calculated recursively using\n",
        "\n",
        "$$\n",
        "    \\alpha(t, u) = \\alpha(t - 1, u) P(\\emptyset | t - 1, u) + \\alpha(t, u - 1) P(y_{u - 1} | t, u - 1)\n",
        "$$\n",
        "\n",
        "with initial condition $\\alpha(1, 0) = 1$. Here $y_{u - 1}$ is the $(u - 1)$-th symbol from the ground truth label $\\mathbf y$.\n",
        "\n",
        "The total output sequene probability is equal to the forward variable at the terminal node:\n",
        "\n",
        "$$\n",
        "    P(\\mathbf y | \\mathbf x) = \\alpha(T, U) P(\\emptyset | T, U)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the *backward variable* $\\beta(t, u)$ as the probability of outputting $\\mathbf y_{[u + 1: U]}$ during $\\mathbf f_{[t:T]}$. Then\n",
        "\n",
        "$$\n",
        "    \\beta(t, u) = \\beta(t + 1, u) P(\\emptyset | t, u) + \\beta(t, u + 1) P(y_u | t, u)\n",
        "$$\n",
        "\n",
        "with initial condition $\\beta(T, U) = P(\\emptyset | T, U)$. The final value is $\\beta(1, 0)$.\n",
        "\n",
        "From the definition of the forward and backward variables it follows that their product $\\alpha(t, u) \\beta(t, u)$ at any point $(t, u)$ in the output lattice is equal to the probability of emitting the complete output sequence *if $y_u$ is emitted during transcription step $t$*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RNN-T Forward-Backward Algorithm (2 points)\n",
        "\n",
        "Implement forward and backward passes.\n",
        "\n",
        "\n",
        "#### Implementation tips\n",
        "\n",
        "- Note that all indices in the arrays you will work with in your code start with zeros. So, the initial condition for forward algorithm will be $\\alpha(0, 0) = 1$ (and $\\log \\alpha(0, 0) = 0$) and the output value for backward algorithm will be $\\beta(0, 0)$. The recurrent formulas stay the same. Also, don't be confused with the terminal node: you don't have to add it to $\\alpha$- and $\\beta$-arrays. The dynamic starts in the upper left corner for forward variables and in the lower right corner for backward variables.\n",
        "- You will need to do everything in log-domain for calculations to be numercally stable. The function [np.logaddexp](https://numpy.org/doc/stable/reference/generated/numpy.logaddexp.html) might help you with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def forward(log_probs: torch.FloatTensor, targets: torch.LongTensor,\n",
        "            blank: int = -1) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
        "    \"\"\"\n",
        "    :param log_probs: model outputs after applying log_softmax\n",
        "    :param targets: the target sequence of tokens, represented as integer indexes\n",
        "    :param blank: the index of blank symbol\n",
        "    :return: Tuple[ln alpha, -(ln alpha(T, U) + ln P(blank | T, U))]. The latter term is loss value, which is -ln P(y | x)\n",
        "    \"\"\"\n",
        "    max_T, max_U, D = log_probs.shape\n",
        "\n",
        "    # here the alpha variable contains logarithm of the alpha variable from the formulas above\n",
        "    alpha = np.zeros((max_T, max_U), dtype=np.float32)\n",
        "\n",
        "    for t in range(1, max_T):\n",
        "        # <YOUR CODE>\n",
        "\n",
        "    for u in range(1, max_U):\n",
        "        # <YOUR CODE>\n",
        "\n",
        "    for t in range(1, max_T):\n",
        "        for u in range(1, max_U):\n",
        "            # <YOUR CODE>\n",
        "\n",
        "    cost = ...  # <YOUR CODE>\n",
        "    return alpha, cost\n",
        "\n",
        "\n",
        "def backward(log_probs: torch.FloatTensor, targets: torch.LongTensor,\n",
        "             blank: int = -1) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
        "    \"\"\"\n",
        "    :param log_probs: model outputs after applying log_softmax\n",
        "    :param targets: the target sequence of tokens, represented as integer indexes\n",
        "    :param blank: the index of blank symbol\n",
        "    :return: Tuple[ln beta, -ln beta(0, 0)]. The latter term is loss value, which is -ln P(y | x)\n",
        "    \"\"\"\n",
        "    max_T, max_U, D = log_probs.shape\n",
        "\n",
        "    # here the beta variable contains logarithm of the beta variable from the formulas above\n",
        "    beta = np.zeros((max_T, max_U), dtype=np.float32)\n",
        "    beta[-1, -1] = log_probs[-1, -1, blank]\n",
        "\n",
        "    for t in reversed(range(max_T - 1)):\n",
        "        # <YOUR CODE>\n",
        "\n",
        "    for u in reversed(range(max_U - 1)):\n",
        "        # <YOUR CODE>\n",
        "\n",
        "    for t in reversed(range(max_T - 1)):\n",
        "        for u in reversed(range(max_U - 1)):\n",
        "            # <YOUR CODE>\n",
        "\n",
        "    cost = ...  # <YOUR CODE>\n",
        "    return beta, cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_test(logits: torch.FloatTensor, targets: torch.LongTensor,\n",
        "             ref_costs: torch.FloatTensor, blank: int = -1) -> None:\n",
        "    \"\"\"\n",
        "    :param logits: model outputs\n",
        "    :param targets: the target sequence of tokens, represented as integer indexes\n",
        "    :param ref_costs: the true values of RNN-T costs for test inputs\n",
        "    :param blank: the index of blank symbol\n",
        "    \"\"\"\n",
        "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "    cost = np.zeros(log_probs.shape[0])\n",
        "\n",
        "    for batch_id in range(log_probs.shape[0]):\n",
        "        alphas, cost_alpha = forward(log_probs[batch_id], targets[batch_id], blank=blank)\n",
        "        betas, cost_beta = backward(log_probs[batch_id], targets[batch_id], blank=blank)\n",
        "        np.testing.assert_almost_equal(cost_alpha, cost_beta, decimal=2)\n",
        "        cost[batch_id] = cost_beta\n",
        "\n",
        "    np.testing.assert_almost_equal(cost, ref_costs, decimal=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tests\n",
        "\n",
        "'''\n",
        "All logits in tests have shapes in the form (B, T, U, D) where\n",
        "\n",
        "B: batch size\n",
        "T: maximum source sequence length in batch\n",
        "U: maximum target sequence length in batch\n",
        "D: feature dimension of each source sequence element\n",
        "'''\n",
        "\n",
        "# test 1\n",
        "logits = torch.FloatTensor([\n",
        "    0.1, 0.6, 0.1, 0.1, 0.1,\n",
        "    0.1, 0.1, 0.6, 0.1, 0.1,\n",
        "    0.1, 0.1, 0.2, 0.8, 0.1,\n",
        "    0.1, 0.6, 0.1, 0.1, 0.1,\n",
        "    0.1, 0.1, 0.2, 0.1, 0.1,\n",
        "    0.7, 0.1, 0.2, 0.1, 0.1,\n",
        "]).reshape(1, 2, 3, 5)\n",
        "\n",
        "targets = torch.LongTensor([[1, 2]])\n",
        "ref_costs = torch.FloatTensor([5.09566688538])\n",
        "\n",
        "run_test(\n",
        "    logits=logits,\n",
        "    targets=targets,\n",
        "    ref_costs=ref_costs,\n",
        "    blank=-1\n",
        ")\n",
        "\n",
        "# test 2\n",
        "logits = torch.FloatTensor([\n",
        "    0.065357, 0.787530, 0.081592, 0.529716, 0.750675, 0.754135, 0.609764, 0.868140,\n",
        "    0.622532, 0.668522, 0.858039, 0.164539, 0.989780, 0.944298, 0.603168, 0.946783,\n",
        "    0.666203, 0.286882, 0.094184, 0.366674, 0.736168, 0.166680, 0.714154, 0.399400,\n",
        "    0.535982, 0.291821, 0.612642, 0.324241, 0.800764, 0.524106, 0.779195, 0.183314,\n",
        "    0.113745, 0.240222, 0.339470, 0.134160, 0.505562, 0.051597, 0.640290, 0.430733,\n",
        "    0.829473, 0.177467, 0.320700, 0.042883, 0.302803, 0.675178, 0.569537, 0.558474,\n",
        "    0.083132, 0.060165, 0.107958, 0.748615, 0.943918, 0.486356, 0.418199, 0.652408,\n",
        "    0.024243, 0.134582, 0.366342, 0.295830, 0.923670, 0.689929, 0.741898, 0.250005,\n",
        "    0.603430, 0.987289, 0.592606, 0.884672, 0.543450, 0.660770, 0.377128, 0.358021,\n",
        "]).reshape(2, 4, 3, 3)\n",
        "\n",
        "targets = torch.LongTensor([[1, 2], [1, 1]])\n",
        "ref_costs = torch.FloatTensor([4.2806528590890736, 3.9384369822503591])\n",
        "\n",
        "run_test(\n",
        "    logits=logits,\n",
        "    targets=targets,\n",
        "    ref_costs=ref_costs,\n",
        "    blank=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BOS = utils.BOS\n",
        "tokenizer = utils.RNNTTokenizer()  # added <BOS> token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download LibriSpeech test dataset\n",
        "\n",
        "if not os.path.isdir(\"./data\"):\n",
        "    os.makedirs(\"./data\")\n",
        "\n",
        "test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"test-clean\", download=True)\n",
        "test_transforms = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collator_fn(data, transforms) -> Tuple[torch.Tensor, torch.IntTensor, torch.IntTensor, torch.IntTensor]:\n",
        "    \"\"\"\n",
        "    :param data: a LIBRISPEECH dataset\n",
        "    :param data_type: \"train\" or \"test\"\n",
        "    :return: tuple of\n",
        "        spectrograms, shape: (B, T, n_mels)\n",
        "        labels, shape: (B, U)\n",
        "        input_lengths -- the length of each spectrogram in the batch, shape: (B,)\n",
        "        label_lengths -- the length of each text label in the batch, shape: (B,)\n",
        "        where\n",
        "        B: batch size\n",
        "        T: maximum source sequence length in batch\n",
        "        U: maximum target sequence length in batch\n",
        "        D: feature dimension of each source sequence element\n",
        "    \"\"\"\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, utterance, _, _, _) in data:\n",
        "        spec = transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.IntTensor(tokenizer.text_to_indices(utterance.lower()))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0])\n",
        "        label_lengths.append(len(label))\n",
        "\n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return spectrograms, torch.IntTensor(labels), torch.IntTensor(input_lengths), torch.IntTensor(label_lengths)\n",
        "\n",
        "test_collator_fn = lambda data: collator_fn(data, test_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementing a greedy decoder (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- <p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1tHsoq0ZH0tHSHYlYlw00y8ksF-wHmrmC\"> -->\n",
        "<p style=\"text-align:center;\"><img src=\"https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_04_asr/images/rnnt_greedy.png\">\n",
        "\n",
        "Now we know how to train a Transducer, but how do we infer it? Our task is to generate an output sequence $\\mathbf y$ given an input acoustic sequence $\\mathbf x$.\n",
        "\n",
        "Here we will index the encoder outputs $f_t$ starting from zero, because it is more convenient when describing an algorithm.\n",
        "\n",
        "The greedy decoding procedure is as follows:\n",
        "1. Compute $\\{f_0, \\ldots, f_T\\}$ using $\\mathbf x$.\n",
        "2. Set $t = 0$, $u = 0$, $\\mathbf y = []$, $\\mathrm{iteration} = 0$.\n",
        "3. If $u = 0$, set $g_0 = \\mathrm{Encoder}(\\langle s \\rangle)$. If $u > 0$, compute $g_u$ using the last predicted token $\\mathbf y[-1]$.\n",
        "4. Compute $P(y | t, u)$ using $f_t$ and $g_u$.\n",
        "5. If argmax of $P(y | t, u)$ is a label, set $u = u + 1$ and append the new label to $\\mathbf y$. \n",
        "6. If argmax of $P(y | t, u)$ is $\\emptyset$, set $t = t + 1$.\n",
        "7. If $t = T$ or $\\mathrm{iteration} = \\mathrm{max\\_iterations}$, we are done. Else, set $\\mathrm{iteration} = \\mathrm{iteration + 1}$ and go to step 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def greedy_decode(model: 'RNNTransducer', encoder_output: torch.Tensor, max_steps: int = 2000) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    :param model: an RNN-T model in eval mode\n",
        "    :param encoder_output: the output of the encoder part of RNN-T, shape: (T, encoder_output_dim)\n",
        "    :param max_steps: the maximum number of decoding steps\n",
        "    :return: the predicted labels\n",
        "    \"\"\"\n",
        "    pred_tokens, hidden_state = [], None\n",
        "    blank = tokenizer.get_symbol_index(BLANK_SYMBOL)\n",
        "    max_time_steps = encoder_output.size(0)\n",
        "    t = 0\n",
        "\n",
        "    decoder_input = encoder_output.new_tensor([[tokenizer.get_symbol_index(BOS)]], dtype=torch.long)\n",
        "    decoder_output, hidden_state = model.decoder(decoder_input, hidden_states=hidden_state)\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        # <YOUR CODE>\n",
        "\n",
        "        if t == max_time_steps:\n",
        "            break\n",
        "\n",
        "    return torch.LongTensor(pred_tokens)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def recognize(model: 'RNNTransducer', inputs: torch.Tensor, input_lengths: torch.Tensor) -> List[torch.Tensor]:\n",
        "    \"\"\"\n",
        "    :param model: an RNN-T model in eval mode\n",
        "    :param inputs: spectrograms, shape: (B, T, n_mels)\n",
        "    :param input_lengths: the lengths of the spectrograms in the batch, shape: (B,)\n",
        "    :return: a list with the predicted labels\n",
        "    \"\"\"\n",
        "    outputs = []\n",
        "    encoder_outputs, _ = model.encoder(inputs, input_lengths)\n",
        "\n",
        "    for encoder_output in encoder_outputs:\n",
        "        decoded_seq = greedy_decode(model, encoder_output)\n",
        "        outputs.append(decoded_seq)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def get_transducer_predictions(\n",
        "        transducer: 'RNNTransducer', inputs: torch.Tensor, input_lengths: torch.Tensor,\n",
        "        targets: torch.Tensor, target_lengths: torch.Tensor\n",
        "    ) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    :param transducer: an RNN-T model in eval mode\n",
        "    :param inputs: spectrograms, shape: (B, T, n_mels)\n",
        "    :param input_lengths: the lengths of the spectrograms in the batch, shape: (B,)\n",
        "    :param targets: labels, shape: (B, U)\n",
        "    :param target_lengths: the lengths of the text labels in the batch, shape: (B,)\n",
        "    :return: a pd.DataFrame with inference results\n",
        "    \"\"\"\n",
        "    predictions = recognize(transducer, inputs, input_lengths)\n",
        "    result = []\n",
        "    for pred, target, target_len in zip(predictions, targets, target_lengths):\n",
        "        label = target[:target_len]\n",
        "        utterance = tokenizer.indices_to_text(list(map(int, label)))\n",
        "        pred_utterance = tokenizer.indices_to_text(list(map(int, pred)))\n",
        "        result.append({\n",
        "            \"ground_truth\": utterance,\n",
        "            \"prediction\": pred_utterance,\n",
        "            \"cer\": utils.cer(utterance, pred_utterance),\n",
        "            \"wer\": utils.wer(utterance, pred_utterance)\n",
        "        })\n",
        "    return pd.DataFrame.from_records(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = torch.jit.load(os.path.join(data_directory, 'model_scripted_epoch_5.pt'))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = data.DataLoader(test_dataset, batch_size=5, shuffle=False, collate_fn=test_collator_fn)\n",
        "spectrograms, labels, input_lengths, label_lengths = next(iter(loader))\n",
        "predictions = get_transducer_predictions(\n",
        "    model, spectrograms, input_lengths,\n",
        "    labels, label_lengths\n",
        ")\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reference_values = [\n",
        "    {\n",
        "        \"gt\": \"he hoped there would be stew for dinner turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick peppered flour fattened sauce\",\n",
        "        \"prediction\": \"he hoped there would be stew for dinner turnips and characts and bruised potatoes and fat much and pieces to be lateled out in the thick peppered flowerfacton sauce\"\n",
        "    },\n",
        "    {\n",
        "        \"gt\": \"stuff it into you his belly counselled him\",\n",
        "        \"prediction\": \"stuffed into you his belly counciled him\"\n",
        "    },\n",
        "    {\n",
        "        \"gt\": \"after early nightfall the yellow lamps would light up here and there the squalid quarter of the brothels\",\n",
        "        \"prediction\": \"after early night fall the yellow lamps would lie how peer and there the squalit quarter of the brothels\"\n",
        "    },\n",
        "    {\n",
        "        \"gt\": \"hello bertie any good in your mind\",\n",
        "        \"prediction\": \"her about he and he good in your mind\"\n",
        "    },\n",
        "    {\n",
        "        \"gt\": \"number ten fresh nelly is waiting on you good night husband\",\n",
        "        \"prediction\": \"none but den fresh now as waiting on you could night husband\"\n",
        "    }\n",
        "]\n",
        "\n",
        "for index in range(5):\n",
        "    gt = predictions.iloc[index].ground_truth\n",
        "    prediction = predictions.iloc[index].prediction\n",
        "    assert gt == reference_values[index][\"gt\"]\n",
        "    assert prediction == reference_values[index][\"prediction\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### RNN-T module (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderRNNT(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_size: int, output_dim: int, n_layers: int,\n",
        "                 dropout: float = 0.2, bidirectional: bool = True):\n",
        "        \"\"\"\n",
        "        An RNN-based model that encodes input audio features into a hidden representation.\n",
        "        The architecture is a stack of LSTM's followed by a fully-connected output layer.\n",
        "\n",
        "        :param input_dim: the number of mel-spectrogram features\n",
        "        :param hidden_size: the number of features in the hidden states in LSTM layers\n",
        "        :param output_dim: the output dimension\n",
        "        :param n_layers: the number of stacked LSTM layers\n",
        "        :param dropout: the dropout probability for LSTM layers\n",
        "        :param bidirectional: If True, each LSTM layer becomes bidirectional\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.lstm = # <YOUR CODE>\n",
        "\n",
        "        self.output_proj = # <YOUR CODE>\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor, input_lengths: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        :param inputs: spectrograms, shape: (B, T, n_mels)\n",
        "        :param input_lengths: the lengths of the spectrograms in the batch, shape: (B,)\n",
        "        :return: outputs of the projection layer and hidden states from LSTMs\n",
        "        \"\"\"\n",
        "        # <YOUR CODE>\n",
        "\n",
        "        return logits, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_pseudo_batch():\n",
        "    spectrograms = nn.utils.rnn.pad_sequence([\n",
        "        torch.rand((835, 80)),\n",
        "        torch.rand((800, 80))\n",
        "    ], batch_first=True)\n",
        "    labels = nn.utils.rnn.pad_sequence([\n",
        "        torch.randint(len(tokenizer.char_map) - 2, (158,)) + 2,\n",
        "        torch.randint(len(tokenizer.char_map) - 2, (150,)) + 2\n",
        "    ], batch_first=True)\n",
        "    input_lengths = torch.IntTensor([835, 800])\n",
        "    label_lengths = torch.IntTensor([158, 150])\n",
        "    return spectrograms, labels, input_lengths, label_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = EncoderRNNT(\n",
        "    input_dim=80,\n",
        "    hidden_size=320,\n",
        "    output_dim=512,\n",
        "    n_layers=4,\n",
        "    dropout=0.2,\n",
        "    bidirectional=True\n",
        ")\n",
        "\n",
        "spectrograms, labels, input_lengths, label_lengths = get_pseudo_batch()\n",
        "logits, hidden_states = encoder.forward(spectrograms, input_lengths)\n",
        "\n",
        "assert spectrograms.shape == torch.Size([2, 835, 80])\n",
        "assert logits.shape == torch.Size([2, 835, 512])\n",
        "assert len(hidden_states) == 2\n",
        "assert hidden_states[0].shape == torch.Size([8, 2, 320])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderRNNT(nn.Module):\n",
        "    def __init__(self, hidden_size: int, vocab_size: int, output_dim: int, n_layers: int, dropout: float = 0.2):\n",
        "        \"\"\"\n",
        "        A simple RNN-based autoregressive language model that takes as input previously generated text tokens\n",
        "        and outputs a hidden representation of the next token\n",
        "\n",
        "        :param hidden_size: the number of features in the hidden states in LSTM layers\n",
        "        :param vocab_size: the number of text tokens in the dictionary\n",
        "        :param output_dim: the output dimension\n",
        "        :param n_layers: the number of stacked LSTM layers\n",
        "        :param dropout: the dropout probability for LSTM layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.lstm = # <YOUR CODE>\n",
        "        self.output_proj = # <YOUR CODE>\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor, input_lengths: Optional[torch.Tensor] = None,\n",
        "                hidden_states: Optional[Tuple[torch.Tensor, torch.Tensor]] = None) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        :param inputs: labels, shape: (B, U)\n",
        "        :param input_lengths: the lengths of the text labels in the batch, shape: (B,)\n",
        "        :return: outputs of the projection layer and hidden states from LSTMs\n",
        "        \"\"\"\n",
        "        embed_inputs = self.embedding(inputs)\n",
        "\n",
        "        if input_lengths is not None:\n",
        "            # training phase, the code here is close to `forward` of the Encoder\n",
        "            # <YOUR CODE>\n",
        "        else:\n",
        "            # testing phase\n",
        "            outputs, hidden = self.lstm(embed_inputs, hidden_states)\n",
        "\n",
        "        outputs = self.output_proj(outputs)\n",
        "        return outputs, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decoder = DecoderRNNT(\n",
        "    hidden_size=512,\n",
        "    vocab_size=len(tokenizer.char_map),\n",
        "    output_dim=512,\n",
        "    n_layers=1,\n",
        "    dropout=0.2\n",
        ")\n",
        "\n",
        "spectrograms, labels, input_lengths, label_lengths = get_pseudo_batch()\n",
        "logits, hidden_states = decoder.forward(labels, label_lengths)\n",
        "\n",
        "assert labels.shape == torch.Size([2, 158])\n",
        "assert logits.shape == torch.Size([2, 158, 512])\n",
        "assert len(hidden_states) == 2\n",
        "assert hidden_states[0].shape == torch.Size([1, 2, 512])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Joiner(nn.Module):\n",
        "    def __init__(self, joiner_dim: int, num_outputs: int):\n",
        "        \"\"\"\n",
        "        Adds encoder and decoder outputs, applies ReLU and passes the result\n",
        "        through a fully connected layer to get the output logits\n",
        "\n",
        "        :param joiner_dim: the dimension of the encoder and decoder outputs\n",
        "        :num_outputs: the number of text tokens in the dictionary\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(joiner_dim, num_outputs)\n",
        "\n",
        "    def forward(self, encoder_outputs: torch.Tensor, decoder_outputs: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        :param encoder_outputs: the encoder outputs (f_t), shape: (B, T, joiner_dim) or (joiner_dim,)\n",
        "        :param decoder_outputs: the decoder outputs (g_u), shape: (B, U, joiner_dim) or (joiner_dim,)\n",
        "        :return: output logits\n",
        "        \"\"\"\n",
        "        if encoder_outputs.dim() == 3 and decoder_outputs.dim() == 3:    # True for training phase\n",
        "            encoder_outputs = encoder_outputs.unsqueeze(2)\n",
        "            decoder_outputs = decoder_outputs.unsqueeze(1)\n",
        "\n",
        "        # Linear(ReLU(f_t + g_u))\n",
        "        out = self.linear(F.relu(encoder_outputs + decoder_outputs))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RNNTransducer(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        num_classes: int,\n",
        "        input_dim: int,\n",
        "        num_encoder_layers: int = 4,\n",
        "        num_decoder_layers: int = 1,\n",
        "        encoder_hidden_state_dim: int = 320,\n",
        "        decoder_hidden_state_dim: int = 512,\n",
        "        output_dim: int = 512,\n",
        "        encoder_is_bidirectional: bool = True,\n",
        "        encoder_dropout_p: float = 0.2,\n",
        "        decoder_dropout_p: float = 0.2\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param num_classes: the number of text tokens in the dictionary\n",
        "        :param input_dim: the number of mel-spectrogram features\n",
        "        :param num_encoder_layers: the number of LSTM layers in the encoder\n",
        "        :param num_decoder_layers: the number of LSTM layers in the decoder\n",
        "        :param encoder_hidden_state_dim: the number of features in the hidden states for the encoder\n",
        "        :param decoder_hidden_state_dim: the number of features in the hidden states for the decoder\n",
        "        :param output_dim: the output dimension\n",
        "        :param encoder_is_bidirectional: whether to use bidirectional LSTM's in the encoder\n",
        "        :param encoder_dropout_p: the dropout probability for the encoder\n",
        "        :param decoder_dropout_p: the dropout probability for the decoder\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.encoder = # <YOUR CODE>\n",
        "\n",
        "        # The decoder takes the input <BOS> + the original sequence.\n",
        "        # You need to shift the current label, and F.pad can help with that.\n",
        "        self.decoder = # <YOUR CODE>\n",
        "        self.joiner = Joiner(output_dim, num_classes)\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor, input_lengths: torch.Tensor,\n",
        "                targets: torch.Tensor, target_lengths: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        :param inputs: spectrograms, shape: (B, T, n_mels)\n",
        "        :param input_lengths: the lengths of the spectrograms in the batch, shape: (B,)\n",
        "        :param targets: labels, shape: (B, U)\n",
        "        :param target_lengths: the lengths of the text labels in the batch, shape: (B,)\n",
        "        :return: the output logits, shape: (B, T, U, n_tokens)\n",
        "        \"\"\"\n",
        "        encoder_outputs, _ = self.encoder(inputs, input_lengths)\n",
        "        # <YOUR CODE>\n",
        "        decoder_outputs, _ = # <YOUR CODE>\n",
        "        joiner_out = self.joiner(encoder_outputs, decoder_outputs)\n",
        "        return joiner_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transducer = RNNTransducer(\n",
        "    num_classes=len(tokenizer.char_map),\n",
        "    input_dim=80,\n",
        "    num_encoder_layers=4,\n",
        "    num_decoder_layers=1,\n",
        "    encoder_hidden_state_dim=320,\n",
        "    decoder_hidden_state_dim=512,\n",
        "    output_dim=512,\n",
        "    encoder_is_bidirectional=True,\n",
        "    encoder_dropout_p=0.2,\n",
        "    decoder_dropout_p=0.2\n",
        ")\n",
        "\n",
        "spectrograms, labels, input_lengths, label_lengths = get_pseudo_batch()\n",
        "result = transducer.forward(spectrograms, input_lengths, labels, label_lengths)\n",
        "\n",
        "assert spectrograms.shape == torch.Size([2, 835, 80])\n",
        "assert labels.shape == torch.Size([2, 158])\n",
        "assert result.shape == torch.Size([2, 835, 159, 30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Examples\n",
        "\n",
        "- Nvidia https://huggingface.co/nvidia/parakeet-rnnt-1.1b\n",
        "- Streaming https://pytorch.org/audio/main/tutorials/online_asr_tutorial.html"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rdORga5D8Chb"
      ],
      "name": "seminar2_student.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
